{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前学習済みの日本語感情分析モデルとそのトークナイザをロード\n",
    "# model_name = 'Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime'\n",
    "model_name = 'patrickramos/bert-base-japanese-v2-wrime-fine-tune'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "\n",
    "# 感情分析のためのパイプラインを設定\n",
    "# nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "# 使用するデバイスの設定\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# モデルをデバイスに転送\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_Φ  = pd.read_parquet('./parquet_data/20250227_tweet_EM_Φ.parquet')\n",
    "words = [re.sub(r'単語\\d+:', '', elem) for elem in pd_Φ.columns]\n",
    "# sentiment_list = [label for label in model.config.id2label.values()]\n",
    "sentiment_list = [\n",
    "\t\t\t\t\t# '主観感情：喜び', '主観感情：悲しみ', '主観感情：期待', '主観感情：驚き', '主観感情：怒り', '主観感情：恐れ', '主観感情：嫌悪', '主観感情：信頼',\n",
    "\t\t\t\t\t'客観感情：喜び', '客観感情：悲しみ', '客観感情：期待', '客観感情：驚き', '客観感情：怒り', '客観感情：恐れ', '客観感情：嫌悪', '客観感情：信頼',\n",
    "\t\t\t\t]\n",
    "sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "# classifier('お、お前が犯人だったのか･･････！')\n",
    "\n",
    "output_size = len(sentiment_list)\n",
    "res_class   = []\n",
    "batch_size  = 64\n",
    "for idx in range(0, len(words), batch_size):\n",
    "\ttargets = words[idx:idx+batch_size]\n",
    "    \n",
    "\tinputs  = tokenizer(targets, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\tinputs  = {key: value.to(device) for key, value in inputs.items()}\n",
    "\toutputs = model(**inputs)\n",
    "\toutputs = torch.maximum(outputs.logits, torch.tensor(0))\n",
    "\tstreng  = outputs[:, 8:8+output_size].tolist()\n",
    " \n",
    "\t# 0:writer_joy, 1:writer_sadness, 2:writer_anticipation, 3:writer_surprise, 4:writer_anger, 5:writer_fear, 6:writer_disgust, 7:writer_trust\n",
    "\tres_class.extend(streng)\n",
    "\n",
    "# 各単語の分類結果の保存\n",
    "pd_sentiment = pd.DataFrame(data=[[a]+b for a,b in zip(words, res_class)], columns=['word'] + sentiment_list)\n",
    "pd_sentiment.to_parquet('./parquet_data/20250227_tweet_sentiment.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMアルゴリズムによる感情分析の結果を保存\n",
    "pd_Φ = pd.read_parquet('./parquet_data/20250227_tweet_EM_Φ.parquet')\n",
    "for label in sentiment_list:\n",
    "\tpd_Φ[label] = 0.0\n",
    "# sentiment_listの分だけpd_Φの列が増えているが、zip関数の挙動的に要素数の少ないres_classに合わせて動作するため問題ない\n",
    "for col, res in zip(pd_Φ.columns, res_class):\n",
    "    # 各単語の感情分析結果を重み付けして感情分析結果を算出\n",
    "\tfor label in sentiment_list:\n",
    "\t\tpd_Φ.loc[:, label] = pd_Φ.loc[:, label] + pd_Φ.loc[:, col] * res[sentiment_list.index(label)]\n",
    "pd_Φ = pd_Φ[[label for label in sentiment_list]]\n",
    "pd_Φ = pd_Φ.div(pd_Φ.sum(axis=1), axis=0)\n",
    "pd_Φ = pd_Φ.round(4)\n",
    "pd_Φ.to_parquet('./parquet_data/20250227_tweet_EM_sentiment.parquet')\n",
    "\n",
    "\n",
    "\n",
    "# 変分ベイズアルゴリズムによる感情分析の結果を保存\n",
    "pd_Φ = pd.read_parquet('./parquet_data/20250227_tweet_VB_Φ.parquet')\n",
    "for label in sentiment_list:\n",
    "\tpd_Φ[label] = 0.0\n",
    "# sentiment_listの分だけpd_Φの列が増えているが、zip関数の挙動的に要素数の少ないres_classに合わせて動作するため問題ない\n",
    "for col, res in zip(pd_Φ.columns, res_class):\n",
    "    # 各単語の感情分析結果を重み付けして感情分析結果を算出\n",
    "\tfor label in sentiment_list:\n",
    "\t\tpd_Φ.loc[:, label] = pd_Φ.loc[:, label] + pd_Φ.loc[:, col] * res[sentiment_list.index(label)]\n",
    "pd_Φ = pd_Φ[[label for label in sentiment_list]]\n",
    "pd_Φ = pd_Φ.div(pd_Φ.sum(axis=1), axis=0)\n",
    "pd_Φ = pd_Φ.round(4)\n",
    "pd_Φ.to_parquet('./parquet_data/20250227_tweet_VB_sentiment.parquet')\n",
    "\n",
    "\n",
    "\n",
    "# ギブスサンプリングアルゴリズムによる感情分析の結果を保存\n",
    "pd_Φ = pd.read_parquet('./parquet_data/20250227_tweet_CGS_Φ.parquet')\n",
    "for label in sentiment_list:\n",
    "\tpd_Φ[label] = 0.0\n",
    "# sentiment_listの分だけpd_Φの列が増えているが、zip関数の挙動的に要素数の少ないres_classに合わせて動作するため問題ない\n",
    "for col, res in zip(pd_Φ.columns, res_class):\n",
    "    # 各単語の感情分析結果を重み付けして感情分析結果を算出\n",
    "\tfor label in sentiment_list:\n",
    "\t\tpd_Φ.loc[:, label] = pd_Φ.loc[:, label] + pd_Φ.loc[:, col] * res[sentiment_list.index(label)]\n",
    "pd_Φ = pd_Φ[[label for label in sentiment_list]]\n",
    "pd_Φ = pd_Φ.div(pd_Φ.sum(axis=1), axis=0)\n",
    "pd_Φ = pd_Φ.round(4)\n",
    "pd_Φ.to_parquet('./parquet_data/20250227_tweet_CGS_sentiment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet  = pd.read_parquet('./parquet_data/20250227_tweet_normalized.parquet')\n",
    "documents = df_tweet['text'].tolist()\n",
    "\n",
    "output_size = len(sentiment_list)\n",
    "res_class   = []\n",
    "res_streng  = []\n",
    "batch_size  = 4\n",
    "for idx in tqdm(range(0, len(documents), batch_size)):\n",
    "\ttargets = documents[idx:idx+batch_size]\n",
    "    \n",
    "\tinputs  = tokenizer(targets, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\tinputs  = {key: value.to(device) for key, value in inputs.items()}\n",
    "\toutputs = model(**inputs)\n",
    "\toutputs = torch.maximum(outputs.logits, torch.tensor(0))\n",
    "\tstreng  = outputs[:, 8:8+output_size]\n",
    "\tresults = [sentiment_list[idx] for idx in torch.argmax(streng, dim=1).tolist()]\n",
    "\tstreng  = streng.tolist()\n",
    " \n",
    "\t# 0:writer_joy, 1:writer_sadness, 2:writer_anticipation, 3:writer_surprise, 4:writer_anger, 5:writer_fear, 6:writer_disgust, 7:writer_trust\n",
    "\tres_streng.extend(streng)\n",
    "\tres_class.extend(results)\n",
    "\t\n",
    "\n",
    "# 各文書の分類結果の保存\n",
    "df_tweet['classification'] = res_class\n",
    "for idx, label in enumerate(sentiment_list):\n",
    "    df_tweet[label] = [elem[idx] for elem in res_streng]\n",
    "\n",
    "df_tweet.to_parquet('./parquet_data/20250227_tweet_normalized.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_vb = pd.read_parquet('./parquet_data/20250227_tweet_VB_sentiment.parquet')\n",
    "pd_vb['sum'] = pd_vb.sum(axis=1)\n",
    "pd_vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
